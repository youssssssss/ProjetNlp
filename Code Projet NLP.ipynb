{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10141385,"sourceType":"datasetVersion","datasetId":6259373}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T20:20:41.226534Z","iopub.execute_input":"2024-12-08T20:20:41.226863Z","iopub.status.idle":"2024-12-08T20:20:41.249863Z","shell.execute_reply.started":"2024-12-08T20:20:41.226833Z","shell.execute_reply":"2024-12-08T20:20:41.248948Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/coarse-and-fine-grained-ner-dataset/coarse-and-fine-grained-ner-dataset.csv\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import ast \nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.read_csv('/kaggle/input/coarse-and-fine-grained-ner-dataset/coarse-and-fine-grained-ner-dataset.csv')\n\nprint(df.head())\n\ndf['Coarse-grained Annotation'] = df['Coarse-grained Annotation'].apply(ast.literal_eval)\n\nsentences = []\nlabels = []\n\nfor _, row in df.iterrows():\n    text = row['Text']\n    annotations = row['Coarse-grained Annotation'] \n\n    token_labels = ['O'] * len(text.split()) \n\n    for start, end, label in annotations:\n        annotated_text = text[start:end].split()\n        for idx, token in enumerate(text.split()):\n            if token in annotated_text:\n                token_labels[idx] = f\"B-{label}\" if idx == 0 else f\"I-{label}\"\n\n    sentences.append(text.split()) \n    labels.append(token_labels)\n\nunique_labels = sorted(set(label for sentence_labels in labels for label in sentence_labels))\nlabel_to_id = {label: idx for idx, label in enumerate(unique_labels)}\nid_to_label = {idx: label for label, idx in label_to_id.items()}\n\nnumerical_labels = [[label_to_id[label] for label in sentence_labels] for sentence_labels in labels]\n\nX_train, X_test, y_train, y_test = train_test_split(sentences, numerical_labels, test_size=0.2, random_state=42)\n\nprint(f\"Number of training sentences: {len(X_train)}\")\nprint(f\"Number of testing sentences: {len(X_test)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T20:20:50.789749Z","iopub.execute_input":"2024-12-08T20:20:50.790608Z","iopub.status.idle":"2024-12-08T20:20:53.102309Z","shell.execute_reply.started":"2024-12-08T20:20:50.790571Z","shell.execute_reply":"2024-12-08T20:20:53.101223Z"}},"outputs":[{"name":"stdout","text":"                                                Text  \\\n0   grandes feuilles opposées, oblongues-elliptiq...   \n1   feuilles opposées, groupées à l'extrémité des...   \n2   feuilles opposées, obovées oblongues, arrondi...   \n3   arbustes  petites feuilles opposées, groupées...   \n4   arbustes  feuilles opposées ou alternes, obla...   \n\n                                      Organ Entities  \\\n0  ['bouton', 'pédicelle', 'corolle', 'tube', 'fe...   \n1  ['limbe', 'style', 'filets', 'rameaux', 'sépal...   \n2  ['corolle', 'limbe', 'ovaire', 'lobes', 'base'...   \n3  ['anthères', 'pétales', 'tube', 'feuilles', 's...   \n4  ['base', 'nervure', 'feuilles', 'arbustes', 'l...   \n\n                                 Descriptor Entities  \\\n0  ['fermée', 'pubes-cents', 'cunéiformes', 'vent...   \n1  ['elliptiques', '1 cm de longueur', 'extrorses...   \n2  ['cunéiforme', '10,5 mm de longueur', 'long', ...   \n3  ['secondaires', 'accusé', 'saillantes', 'apicu...   \n4  ['proéminente', 'décurrente', 'alternes', 'obl...   \n\n                           Coarse-grained Annotation  \\\n0  [(650, 661, 'DESCRIPTEUR'), (968, 977, 'DESCRI...   \n1  [(609, 618, 'DESCRIPTEUR'), (129, 144, 'DESCRI...   \n2  [(60, 64, 'ORGANE'), (196, 205, 'DESCRIPTEUR')...   \n3  [(949, 959, 'DESCRIPTEUR'), (88, 105, 'DESCRIP...   \n4  [(140, 150, 'DESCRIPTEUR'), (32, 40, 'DESCRIPT...   \n\n                             Fine-grained Annotation  \n0  [(650, 661, 'DESCRIPTEUR'), (395, 407, 'DISPOS...  \n1  [(609, 618, 'DESCRIPTEUR'), (73, 80, 'FORME'),...  \n2  [(60, 64, 'ORGANE'), (180, 187, 'ORGANE'), (11...  \n3  [(949, 959, 'DESCRIPTEUR'), (88, 105, 'DESCRIP...  \n4  [(42, 54, 'FORME'), (119, 129, 'FORME'), (1, 9...  \nNumber of training sentences: 670\nNumber of testing sentences: 168\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"from transformers import BertTokenizerFast\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch\n\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\nmax_len = 128 \nbatch_size = 16\n\nclass NERDataset(Dataset):\n    def __init__(self, sentences, labels, tokenizer, max_len):\n        self.sentences = sentences\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, idx):\n        encoded = self.tokenizer(\n            self.sentences[idx],\n            is_split_into_words=True,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_len,\n            return_tensors=\"pt\"\n        )\n        \n        word_ids = encoded.word_ids() \n        label_ids = []\n        for word_id in word_ids:\n            if word_id is None: \n                label_ids.append(-100) \n            else:\n                label_ids.append(self.labels[idx][word_id])\n\n        return {\n            'input_ids': encoded['input_ids'].squeeze(),\n            'attention_mask': encoded['attention_mask'].squeeze(),\n            'labels': torch.tensor(label_ids)\n        }\n\n# creating datasets and DataLoaders\ntrain_dataset = NERDataset(X_train, y_train, tokenizer, max_len)\ntest_dataset = NERDataset(X_test, y_test, tokenizer, max_len)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n\nfor batch in train_loader:\n    print(batch.keys())\n    print(batch['input_ids'].shape, batch['attention_mask'].shape, batch['labels'].shape)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T20:20:57.235658Z","iopub.execute_input":"2024-12-08T20:20:57.236006Z","iopub.status.idle":"2024-12-08T20:20:57.465604Z","shell.execute_reply.started":"2024-12-08T20:20:57.235975Z","shell.execute_reply":"2024-12-08T20:20:57.464719Z"}},"outputs":[{"name":"stdout","text":"dict_keys(['input_ids', 'attention_mask', 'labels'])\ntorch.Size([16, 128]) torch.Size([16, 128]) torch.Size([16, 128])\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"from transformers import BertForTokenClassification\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nnum_labels = len(label_to_id)\nmodel = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_labels).to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss(ignore_index=-100)\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n\n# model\nnum_labels = len(label_to_id) \nmodel = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_labels).to(device)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=-100)\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n\n# Training\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0\n    for batch in train_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n    avg_train_loss = train_loss / len(train_loader)\n    print(f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}\")\n\n# Saving model\nmodel_save_path = \"bert_ner_model.pth\"\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model saved to {model_save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T20:20:59.946380Z","iopub.execute_input":"2024-12-08T20:20:59.947165Z","iopub.status.idle":"2024-12-08T20:21:48.538164Z","shell.execute_reply.started":"2024-12-08T20:20:59.947110Z","shell.execute_reply":"2024-12-08T20:21:48.536812Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/5], Training Loss: 0.5573\nEpoch [2/5], Training Loss: 0.1867\nEpoch [3/5], Training Loss: 0.1041\nEpoch [4/5], Training Loss: 0.0721\nEpoch [5/5], Training Loss: 0.0526\nModel saved to bert_ner_model.pth\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nmodel.eval()\nall_predictions = []\nall_labels = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        predictions = torch.argmax(outputs.logits, dim=-1)\n\n        active_labels = labels[labels != -100]\n        active_predictions = predictions[labels != -100]\n\n        all_predictions.extend(active_predictions.cpu().numpy())\n        all_labels.extend(active_labels.cpu().numpy())\n\nreport = classification_report(all_labels, all_predictions, target_names=list(label_to_id.keys()))\nprint(\"Classification Report:\\n\", report)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T20:21:51.377205Z","iopub.execute_input":"2024-12-08T20:21:51.377516Z","iopub.status.idle":"2024-12-08T20:21:52.303201Z","shell.execute_reply.started":"2024-12-08T20:21:51.377491Z","shell.execute_reply":"2024-12-08T20:21:52.302300Z"}},"outputs":[{"name":"stdout","text":"Classification Report:\n                precision    recall  f1-score   support\n\nB-DESCRIPTEUR       0.00      0.00      0.00         1\n     B-ORGANE       1.00      0.99      1.00       380\nI-DESCRIPTEUR       0.88      0.91      0.90      2521\n     I-ORGANE       0.96      0.97      0.96      2604\n            O       0.98      0.97      0.98     14932\n\n     accuracy                           0.97     20438\n    macro avg       0.76      0.77      0.77     20438\n weighted avg       0.97      0.97      0.97     20438\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"The classification report reveals promising performance for most labels, but there are areas that need improvement:\n\nObservations\nStrong Performance:\n\nLabels like I-ORGANE, B-ORGANE, and O show high precision, recall, and F1-scores, indicating that the model effectively recognizes these classes.\nWeak Performance:\n\nLabels such as B-DESCRIPTEUR have no predicted samples, resulting in undefined precision and F1-scores.\nImbalance Challenge:\n\nThe label B-DESCRIPTEUR appears to be significantly underrepresented in the dataset (only 1 instance in the test set).\nOverall Performance:\n\nThe macro-average F1-score is affected by the underperformance on rare classes.\nThe weighted average F1-score is high because it is dominated by the majority classes.\nSuggestions for Improvement\nHandle Imbalance in Rare Classes:\n\nAugment the dataset to include more instances of underrepresented classes like B-DESCRIPTEUR.\nUse class weights in the loss function to give more importance to rare classes.","metadata":{}},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\nclass_weights = compute_class_weight('balanced', classes=list(label_to_id.values()), y=all_labels)\nclass_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n\ncriterion = nn.CrossEntropyLoss(weight=class_weights_tensor, ignore_index=-100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T20:22:00.105741Z","iopub.execute_input":"2024-12-08T20:22:00.106753Z","iopub.status.idle":"2024-12-08T20:22:00.117339Z","shell.execute_reply.started":"2024-12-08T20:22:00.106698Z","shell.execute_reply":"2024-12-08T20:22:00.116384Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"from torch.optim.lr_scheduler import StepLR\nscheduler = StepLR(optimizer, step_size=2, gamma=0.5)  # Decay learning rate every 2 epochs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T20:22:04.785562Z","iopub.execute_input":"2024-12-08T20:22:04.786471Z","iopub.status.idle":"2024-12-08T20:22:04.792944Z","shell.execute_reply.started":"2024-12-08T20:22:04.786431Z","shell.execute_reply":"2024-12-08T20:22:04.792247Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\n\n# Flatten all labels in the training set to compute class frequencies\nall_train_labels = np.concatenate([label for label in y_train])\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(all_train_labels),\n    y=all_train_labels\n)\n\n# Convert to tensor for PyTorch\nclass_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n\nprint(\"Class weights:\", class_weights_tensor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T20:22:17.078418Z","iopub.execute_input":"2024-12-08T20:22:17.078759Z","iopub.status.idle":"2024-12-08T20:22:17.109357Z","shell.execute_reply.started":"2024-12-08T20:22:17.078729Z","shell.execute_reply":"2024-12-08T20:22:17.108393Z"}},"outputs":[{"name":"stdout","text":"Class weights: tensor([2.7116e+04, 5.2247e+01, 1.4692e+00, 1.7624e+00, 2.6790e-01],\n       device='cuda:0')\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, ignore_index=-100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T20:22:20.710034Z","iopub.execute_input":"2024-12-08T20:22:20.710827Z","iopub.status.idle":"2024-12-08T20:22:20.714773Z","shell.execute_reply.started":"2024-12-08T20:22:20.710794Z","shell.execute_reply":"2024-12-08T20:22:20.713822Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# train model\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0\n    for batch in train_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n    avg_train_loss = train_loss / len(train_loader)\n    print(f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}\")\n\n# Saving model\nmodel_save_path = \"bert_ner_model_with_weights.pth\"\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model saved to {model_save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T20:22:24.294378Z","iopub.execute_input":"2024-12-08T20:22:24.294739Z","iopub.status.idle":"2024-12-08T20:23:11.762331Z","shell.execute_reply.started":"2024-12-08T20:22:24.294708Z","shell.execute_reply":"2024-12-08T20:23:11.760820Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/5], Training Loss: 0.0410\nEpoch [2/5], Training Loss: 0.0321\nEpoch [3/5], Training Loss: 0.0239\nEpoch [4/5], Training Loss: 0.0203\nEpoch [5/5], Training Loss: 0.0196\nModel saved to bert_ner_model_with_weights.pth\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Load model\nmodel.load_state_dict(torch.load(\"bert_ner_model_with_weights.pth\"))\nmodel.eval()\n\n# Evaluation\nall_predictions = []\nall_labels = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        predictions = torch.argmax(outputs.logits, dim=-1)\n\n        # Collect predictions and labels ignoring padding tokens (-100)\n        active_labels = labels[labels != -100]\n        active_predictions = predictions[labels != -100]\n\n        all_predictions.extend(active_predictions.cpu().numpy())\n        all_labels.extend(active_labels.cpu().numpy())\n\n# Generate a report\nreport = classification_report(all_labels, all_predictions, target_names=list(label_to_id.keys()))\nprint(\"Classification Report:\\n\", report)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T20:23:20.668658Z","iopub.execute_input":"2024-12-08T20:23:20.669017Z","iopub.status.idle":"2024-12-08T20:23:21.889555Z","shell.execute_reply.started":"2024-12-08T20:23:20.668985Z","shell.execute_reply":"2024-12-08T20:23:21.888681Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/3820475007.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"bert_ner_model_with_weights.pth\"))\n","output_type":"stream"},{"name":"stdout","text":"Classification Report:\n                precision    recall  f1-score   support\n\nB-DESCRIPTEUR       0.00      0.00      0.00         1\n     B-ORGANE       1.00      1.00      1.00       380\nI-DESCRIPTEUR       0.86      0.95      0.90      2521\n     I-ORGANE       0.96      0.97      0.97      2604\n            O       0.99      0.97      0.98     14932\n\n     accuracy                           0.97     20438\n    macro avg       0.76      0.78      0.77     20438\n weighted avg       0.97      0.97      0.97     20438\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"Updated Evaluation Analysis\nThe re-training with class weights has improved the model’s performance slightly, but there are still issues with rare classes:\n\nObservations\nMajority Classes (B-ORGANE, I-ORGANE, O):\n\nThese classes maintain high precision, recall, and F1-scores, indicating robust performance.\nUnderrepresented Class (B-DESCRIPTEUR):\n\nThis class still has no predicted samples, leading to undefined precision and F1-scores.\nOverall Metrics:\n\nThe macro average F1-score remains limited due to poor performance on rare classes.\nThe weighted average F1-score is high, reflecting dominance by majority classes.\nSuggestions for Further Improvement\n1. Data Augmentation\nGenerate synthetic training samples for rare classes like B-DESCRIPTEUR to improve representation.\n2. Over-Sampling Rare Classes\nDuplicate instances of rare classes in the training set to give them more weight during training.\n3. Fine-Grained Class Balancing\nAdd dynamic sampling or more aggressive weighting for extremely rare classes.\n4. Alternative Architectures\nExperiment with models such as RoBERTa or DeBERTa, which may handle rare class features better.\n5. Custom Loss Functions","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T20:17:38.424253Z","iopub.execute_input":"2024-12-08T20:17:38.424607Z","iopub.status.idle":"2024-12-08T20:17:38.429920Z","shell.execute_reply.started":"2024-12-08T20:17:38.424576Z","shell.execute_reply":"2024-12-08T20:17:38.429049Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Load model\nmodel.load_state_dict(torch.load(\"bert_ner_model_with_weights.pth\"))\nmodel.eval()\n\n# Evaluation\nall_predictions = []\nall_labels = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        predictions = torch.argmax(outputs.logits, dim=-1)\n\n        # Collect predictions and labels ignoring padding tokens (-100)\n        active_labels = labels[labels != -100]\n        active_predictions = predictions[labels != -100]\n\n        all_predictions.extend(active_predictions.cpu().numpy())\n        all_labels.extend(active_labels.cpu().numpy())\n\n# Generate a report\nreport = classification_report(all_labels, all_predictions, target_names=list(label_to_id.keys()))\nprint(\"Classification Report:\\n\", report)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T20:32:29.645451Z","iopub.execute_input":"2024-12-08T20:32:29.645815Z","iopub.status.idle":"2024-12-08T20:32:30.918267Z","shell.execute_reply.started":"2024-12-08T20:32:29.645784Z","shell.execute_reply":"2024-12-08T20:32:30.917358Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/3820475007.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"bert_ner_model_with_weights.pth\"))\n","output_type":"stream"},{"name":"stdout","text":"Classification Report:\n                precision    recall  f1-score   support\n\nB-DESCRIPTEUR       0.00      0.00      0.00         1\n     B-ORGANE       1.00      1.00      1.00       380\nI-DESCRIPTEUR       0.86      0.95      0.90      2521\n     I-ORGANE       0.96      0.97      0.97      2604\n            O       0.99      0.97      0.98     14932\n\n     accuracy                           0.97     20438\n    macro avg       0.76      0.78      0.77     20438\n weighted avg       0.97      0.97      0.97     20438\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"Let's focus on data augmentation to improve the representation of rare classes, particularly B-DESCRIPTEUR. Here’s how to proceed:\n\nStep 1: Oversample Rare Classes\nDuplicate instances of rare classes in the training set to balance the class distribution.\n","metadata":{}},{"cell_type":"code","source":"import random\nfrom collections import Counter\n\nflat_labels = [label for sentence_labels in y_train for label in sentence_labels]\nlabel_counts = Counter(flat_labels)\nprint(\"Class Distribution Before Oversampling:\", label_counts)\n\nmin_count = max(label_counts.values())\naugmented_sentences = []\naugmented_labels = []\n\nfor sentence, labels in zip(X_train, y_train):\n    if any(label_counts[label] < min_count for label in labels):\n        multiplier = min_count // label_counts[labels[0]]\n        for _ in range(multiplier):\n            augmented_sentences.append(sentence)\n            augmented_labels.append(labels)\n\nX_train_augmented = X_train + augmented_sentences\ny_train_augmented = y_train + augmented_labels\n\nprint(f\"Original Training Samples: {len(X_train)}, Augmented Samples: {len(X_train_augmented)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T20:32:36.940468Z","iopub.execute_input":"2024-12-08T20:32:36.940803Z","iopub.status.idle":"2024-12-08T20:32:36.999301Z","shell.execute_reply.started":"2024-12-08T20:32:36.940774Z","shell.execute_reply":"2024-12-08T20:32:36.998421Z"}},"outputs":[{"name":"stdout","text":"Class Distribution Before Oversampling: Counter({4: 101219, 2: 18457, 3: 15386, 1: 519, 0: 1})\nOriginal Training Samples: 670, Augmented Samples: 203244\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"# creating augmented dataset\ntrain_dataset_augmented = NERDataset(X_train_augmented, y_train_augmented, tokenizer, max_len)\ntrain_loader_augmented = DataLoader(train_dataset_augmented, batch_size=batch_size, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T20:32:40.997360Z","iopub.execute_input":"2024-12-08T20:32:40.998188Z","iopub.status.idle":"2024-12-08T20:32:41.013000Z","shell.execute_reply.started":"2024-12-08T20:32:40.998150Z","shell.execute_reply":"2024-12-08T20:32:41.011971Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"from transformers import BertForTokenClassification, AdamW\nfrom torch.optim.lr_scheduler import StepLR\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Subset\n\nsubset_indices = list(range(100)) \ntrain_subset = Subset(train_dataset, subset_indices)\ntrain_loader = DataLoader(train_subset, batch_size=16, shuffle=True)\n\nnum_labels = len(label_to_id)\nmodel = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_labels).to(device)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=-100) \noptimizer = AdamW(model.parameters(), lr=5e-5)\nscheduler = StepLR(optimizer, step_size=1, gamma=0.5) \n\nnum_epochs = 9\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0\n    for batch in train_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n    scheduler.step()\n\n    avg_train_loss = train_loss / len(train_loader)\n    print(f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}\")\n\nmodel_save_path = \"efficient_bert_ner_model.pth\"\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model saved to {model_save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T21:12:11.004142Z","iopub.execute_input":"2024-12-08T21:12:11.004498Z","iopub.status.idle":"2024-12-08T21:12:25.213844Z","shell.execute_reply.started":"2024-12-08T21:12:11.004469Z","shell.execute_reply":"2024-12-08T21:12:25.212634Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/9], Training Loss: 0.9475\nEpoch [2/9], Training Loss: 0.7174\nEpoch [3/9], Training Loss: 0.6303\nEpoch [4/9], Training Loss: 0.5874\nEpoch [5/9], Training Loss: 0.5593\nEpoch [6/9], Training Loss: 0.5482\nEpoch [7/9], Training Loss: 0.5459\nEpoch [8/9], Training Loss: 0.5375\nEpoch [9/9], Training Loss: 0.5371\nModel saved to efficient_bert_ner_model.pth\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nmodel.load_state_dict(torch.load(\"efficient_bert_ner_model.pth\"))\nmodel.eval()\n\nall_predictions = []\nall_labels = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        # Forward pass\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        predictions = torch.argmax(outputs.logits, dim=-1)\n\n        # Collect predictions and labels ignoring padding tokens (-100)\n        active_labels = labels[labels != -100]\n        active_predictions = predictions[labels != -100]\n\n        all_predictions.extend(active_predictions.cpu().numpy())\n        all_labels.extend(active_labels.cpu().numpy())\n\nreport = classification_report(all_labels, all_predictions, target_names=list(label_to_id.keys()))\nprint(\"Classification Report:\\n\", report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T21:12:54.216932Z","iopub.execute_input":"2024-12-08T21:12:54.217324Z","iopub.status.idle":"2024-12-08T21:12:55.447148Z","shell.execute_reply.started":"2024-12-08T21:12:54.217293Z","shell.execute_reply":"2024-12-08T21:12:55.446271Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/4143663106.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"efficient_bert_ner_model.pth\"))\n","output_type":"stream"},{"name":"stdout","text":"Classification Report:\n                precision    recall  f1-score   support\n\nB-DESCRIPTEUR       0.00      0.00      0.00         1\n     B-ORGANE       0.00      0.00      0.00       380\nI-DESCRIPTEUR       0.17      0.04      0.06      2521\n     I-ORGANE       0.70      0.70      0.70      2604\n            O       0.83      0.96      0.89     14932\n\n     accuracy                           0.80     20438\n    macro avg       0.34      0.34      0.33     20438\n weighted avg       0.72      0.80      0.75     20438\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":48},{"cell_type":"markdown","source":"Analysis of Evaluation Results\nObservations:\nStrong Performance on Majority Class (O):\n\nThe model achieves high precision (0.83) and recall (0.96) for the O class, which dominates the dataset.\nMixed Results for Other Classes:\n\nI-ORGANE performs moderately well, with an F1-score of 0.70.\nI-DESCRIPTEUR shows poor performance, with an F1-score of 0.06.\nThe B-ORGANE and B-DESCRIPTEUR classes have no predictions (0.00 precision and recall).\nImbalanced Dataset Effect:\n\nThe model struggles with rare classes (B-DESCRIPTEUR and B-ORGANE) due to insufficient examples during training.\nOverall Metrics:\n\nThe macro-average F1-score is 0.33, highlighting the model's difficulty with underrepresented classes.\nWeighted average F1-score is 0.75, heavily influenced by the dominant O class.","metadata":{}},{"cell_type":"markdown","source":"Evaluate and Compare Models","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\ndef evaluate_model(model_path, test_loader, label_to_id):\n    model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(label_to_id))\n    model.load_state_dict(torch.load(model_path))\n    model.to(device)\n    model.eval()\n    \n    all_predictions = []\n    all_labels = []\n    with torch.no_grad():\n        for batch in test_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            predictions = torch.argmax(outputs.logits, dim=-1)\n\n            active_labels = labels[labels != -100]\n            active_predictions = predictions[labels != -100]\n\n            all_predictions.extend(active_predictions.cpu().numpy())\n            all_labels.extend(active_labels.cpu().numpy())\n\n    report = classification_report(all_labels, all_predictions, target_names=list(label_to_id.keys()), output_dict=True)\n    return report\n\nmodel_paths = [\n    \"/kaggle/working/bert_ner_model.pth\",\n    \"/kaggle/working/bert_ner_model_with_weights.pth\",\n    \"/kaggle/working/efficient_bert_ner_model.pth\"\n]\n\nmodel_names = [\"Base Model\", \"Model with Class Weights\", \"Efficient Model\"]\n\nresults = {}\nfor name, path in zip(model_names, model_paths):\n    print(f\"Evaluating: {name}\")\n    report = evaluate_model(path, test_loader, label_to_id)\n    results[name] = report\n    print(f\"{name} - Classification Report:\")\n    for label, metrics in report.items():\n        if label in label_to_id.keys():\n            print(f\"{label}: Precision={metrics['precision']:.2f}, Recall={metrics['recall']:.2f}, F1-Score={metrics['f1-score']:.2f}\")\n    print()\n\nprint(\"Comparison Summary:\")\nfor label in label_to_id.keys():\n    print(f\"\\nLabel: {label}\")\n    for name in model_names:\n        metrics = results[name][label]\n        print(f\"{name}: Precision={metrics['precision']:.2f}, Recall={metrics['recall']:.2f}, F1-Score={metrics['f1-score']:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T21:16:28.407165Z","iopub.execute_input":"2024-12-08T21:16:28.407803Z","iopub.status.idle":"2024-12-08T21:16:34.813364Z","shell.execute_reply.started":"2024-12-08T21:16:28.407768Z","shell.execute_reply":"2024-12-08T21:16:34.812521Z"}},"outputs":[{"name":"stdout","text":"Evaluating: Base Model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_23/269034089.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Base Model - Classification Report:\nB-DESCRIPTEUR: Precision=0.00, Recall=0.00, F1-Score=0.00\nB-ORGANE: Precision=1.00, Recall=0.99, F1-Score=1.00\nI-DESCRIPTEUR: Precision=0.88, Recall=0.91, F1-Score=0.90\nI-ORGANE: Precision=0.96, Recall=0.97, F1-Score=0.96\nO: Precision=0.98, Recall=0.97, F1-Score=0.98\n\nEvaluating: Model with Class Weights\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_23/269034089.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Model with Class Weights - Classification Report:\nB-DESCRIPTEUR: Precision=0.00, Recall=0.00, F1-Score=0.00\nB-ORGANE: Precision=1.00, Recall=1.00, F1-Score=1.00\nI-DESCRIPTEUR: Precision=0.86, Recall=0.95, F1-Score=0.90\nI-ORGANE: Precision=0.96, Recall=0.97, F1-Score=0.97\nO: Precision=0.99, Recall=0.97, F1-Score=0.98\n\nEvaluating: Efficient Model\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_23/269034089.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n","output_type":"stream"},{"name":"stdout","text":"Efficient Model - Classification Report:\nB-DESCRIPTEUR: Precision=0.00, Recall=0.00, F1-Score=0.00\nB-ORGANE: Precision=0.00, Recall=0.00, F1-Score=0.00\nI-DESCRIPTEUR: Precision=0.17, Recall=0.04, F1-Score=0.06\nI-ORGANE: Precision=0.70, Recall=0.70, F1-Score=0.70\nO: Precision=0.83, Recall=0.96, F1-Score=0.89\n\nComparison Summary:\n\nLabel: B-DESCRIPTEUR\nBase Model: Precision=0.00, Recall=0.00, F1-Score=0.00\nModel with Class Weights: Precision=0.00, Recall=0.00, F1-Score=0.00\nEfficient Model: Precision=0.00, Recall=0.00, F1-Score=0.00\n\nLabel: B-ORGANE\nBase Model: Precision=1.00, Recall=0.99, F1-Score=1.00\nModel with Class Weights: Precision=1.00, Recall=1.00, F1-Score=1.00\nEfficient Model: Precision=0.00, Recall=0.00, F1-Score=0.00\n\nLabel: I-DESCRIPTEUR\nBase Model: Precision=0.88, Recall=0.91, F1-Score=0.90\nModel with Class Weights: Precision=0.86, Recall=0.95, F1-Score=0.90\nEfficient Model: Precision=0.17, Recall=0.04, F1-Score=0.06\n\nLabel: I-ORGANE\nBase Model: Precision=0.96, Recall=0.97, F1-Score=0.96\nModel with Class Weights: Precision=0.96, Recall=0.97, F1-Score=0.97\nEfficient Model: Precision=0.70, Recall=0.70, F1-Score=0.70\n\nLabel: O\nBase Model: Precision=0.98, Recall=0.97, F1-Score=0.98\nModel with Class Weights: Precision=0.99, Recall=0.97, F1-Score=0.98\nEfficient Model: Precision=0.83, Recall=0.96, F1-Score=0.89\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"from torch.utils.data import ConcatDataset\n\nb_descripteur_indices = [i for i, labels in enumerate(y_train) if 'B-DESCRIPTEUR' in labels]\nb_descripteur_subset = Subset(train_dataset, b_descripteur_indices)\n\naugmented_dataset = ConcatDataset([train_dataset, b_descripteur_subset] * 10)  # Multiply by 10\ntrain_loader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T21:20:31.602510Z","iopub.execute_input":"2024-12-08T21:20:31.602922Z","iopub.status.idle":"2024-12-08T21:20:31.610096Z","shell.execute_reply.started":"2024-12-08T21:20:31.602872Z","shell.execute_reply":"2024-12-08T21:20:31.609069Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"num_epochs = 5\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0\n    for batch in train_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n    avg_train_loss = train_loss / len(train_loader)\n    print(f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}\")\n\nmodel_save_path = \"model_balanced.pth\"\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model saved to {model_save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T21:25:29.955342Z","iopub.execute_input":"2024-12-08T21:25:29.955950Z","iopub.status.idle":"2024-12-08T21:34:26.006348Z","shell.execute_reply.started":"2024-12-08T21:25:29.955916Z","shell.execute_reply":"2024-12-08T21:34:26.005446Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/5], Training Loss: 0.4449\nEpoch [2/5], Training Loss: 0.4031\nEpoch [3/5], Training Loss: 0.3678\nEpoch [4/5], Training Loss: 0.3380\nEpoch [5/5], Training Loss: 0.3115\nModel saved to model_balanced.pth\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"evaluate_model(\"model_balanced.pth\", test_loader, label_to_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T21:35:01.172984Z","iopub.execute_input":"2024-12-08T21:35:01.173363Z","iopub.status.idle":"2024-12-08T21:35:03.250760Z","shell.execute_reply.started":"2024-12-08T21:35:01.173333Z","shell.execute_reply":"2024-12-08T21:35:03.249965Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_23/269034089.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"{'B-DESCRIPTEUR': {'precision': 0.0,\n  'recall': 0.0,\n  'f1-score': 0.0,\n  'support': 1},\n 'B-ORGANE': {'precision': 0.890818858560794,\n  'recall': 0.9447368421052632,\n  'f1-score': 0.9169859514687101,\n  'support': 380},\n 'I-DESCRIPTEUR': {'precision': 0.6605966679581557,\n  'recall': 0.6763189210630702,\n  'f1-score': 0.6683653469227754,\n  'support': 2521},\n 'I-ORGANE': {'precision': 0.844559585492228,\n  'recall': 0.9389400921658986,\n  'f1-score': 0.889252591380251,\n  'support': 2604},\n 'O': {'precision': 0.9423037296517618,\n  'recall': 0.9187650683096705,\n  'f1-score': 0.9303855413515987,\n  'support': 14932},\n 'accuracy': 0.8918680888540953,\n 'macro avg': {'precision': 0.6676557683325879,\n  'recall': 0.6957521847287805,\n  'f1-score': 0.680997886224667,\n  'support': 20438},\n 'weighted avg': {'precision': 0.8940986308815676,\n  'recall': 0.8918680888540953,\n  'f1-score': 0.8925303039713608,\n  'support': 20438}}"},"metadata":{}}],"execution_count":56},{"cell_type":"markdown","source":"Testing the model :","metadata":{}},{"cell_type":"markdown","source":"Retrain and evaluation :","metadata":{}}]}